[[Deep Learning.canvas]]

> [!info] 
> Im using obsidian. So, before i actually start studying, generate the headings for all the sources that have been selected. (Do not make unnecessary headings). Im making these notes to understand the topics
> 
> In this format:
> 
> # Lecture 5: Perceptron Learning Rule
> 
> ## Learning from Errors and Weight Updates
> 
> _(Notes on Hebb's rule and shifting decision boundaries)_
> 
> ## Perceptron Convergence Theorem
> 
> _(Notes on the theoretical guarantees for linearly separable data)_
> 
> ## Limitations of the Perceptron Rule
> 
> _(Notes on what happens when data isn't linearly separable and the need for smooth, quantitative error measures)_
> 
> ## Weight Initialization Strategies and Pitfalls
> 
> _(Notes on zero initialization, He initialization, vanishing/exploding gradients, and the symmetry problem)_




